\documentclass{article}
\setlength\parindent{0pt}
\setlength{\parskip}{0pt}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage[linesnumbered,boxed]{algorithm2e}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\SetKwInput{KwInt}{Initialize}
\usepackage{mathrsfs}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{geometry}
\geometry{a4paper,scale=0.8}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{ulem}
\linespread{1.5}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}


\begin{document}

\title{Review of Independent Component Analysis\\ \small{Element of Statistical Learning Book, Chapter 14}}

\author{Hanchao Zhang}

\maketitle

\section{Latent Variables an Factor Analysis}

The singular decomposition 

\begin{equation}
	\pmb X = \pmb U \pmb D \pmb V^T
\end{equation}

We can write $\pmb S = \sqrt N \pmb U$ and $\pmb A^T = \frac{\pmb D\pmb V^T}{\sqrt{N}}$

and we have 

\begin{equation}
	\pmb X = \pmb S \pmb A^T = \sqrt N \pmb U \frac{\pmb D\pmb V^T}{\sqrt{N}} = \pmb U \pmb D \pmb V^T
\end{equation}

Where $\pmb S$ and $\pmb X$ have mean $0$, and $\pmb U$ is an orthogonal matrix. We can interpret the SVD or the corresponding principal component analysis as an estimate of a latent variable model.
\begin{equation}
\begin{aligned}
	X_1 &= a_{11}S_1 + a_{12}S_2 + \cdots + a_{1p} S_p\\
	X_2 &= a_{21}S_1 + a_{22}S_2 + \cdots + a_{2p} S_p\\
	\vdots & \hspace{30mm} \vdots \\
	X_p &= a_{p1}S_1 + a_{p2}S_2 + \cdots + a_{pp} S_p
\end{aligned}
\end{equation}
However, for any orthogonal matrix $\pmb R$, we can write

\begin{equation}
\begin{aligned}
	X & = \pmb A S \\
	& = \pmb A \pmb R^T \pmb R S \\
	& = \pmb A^* S^*
\end{aligned}
\end{equation}

Hence there are many such decompositions and it is therefore impossible to identify any particular latent variable as unique underlying sources. The classical factor analysis model has the form ($q<p$)

\begin{equation}
\begin{aligned}
	X_1 &= a_{11}S_1 + a_{12}S_2 + \cdots + a_{1q} S_q + \varepsilon_1\\
	X_2 &= a_{21}S_1 + a_{22}S_2 + \cdots + a_{2q} S_q + \varepsilon_2\\
	\vdots & \hspace{30mm} \vdots \\
	X_p &= a_{p1}S_1 + a_{p2}S_2 + \cdots + a_{pq} S_q + \varepsilon_p
\end{aligned}
\end{equation}

or 

\begin{equation}
	X = \pmb A S + \varepsilon
\end{equation}


Typically the $S_j$ and $\varepsilon_j$ are modeled as Gaussian random variables and the model is fit by maximum likelihood.

\section{Independent Component Analysis}

The ICA model has the form:

\begin{equation}
\begin{aligned}
	X_1 &= a_{11}S_1 + a_{12}S_2 + \cdots + a_{1p} S_p\\
	X_2 &= a_{21}S_1 + a_{22}S_2 + \cdots + a_{2p} S_p\\
	\vdots & \hspace{30mm} \vdots \\
	X_p &= a_{p1}S_1 + a_{p2}S_2 + \cdots + a_{pp} S_p
\end{aligned}
\end{equation}

or

\begin{equation}
	X = \pmb A S
\end{equation}

where the $S_i$ are assumed to be statistically independent rather than uncorrelated. Intuitively, uncorrelated determines the second cross moment $\text{Cov(X)}$, and statistically independent determines all cross moments.

We wish to recover the matrix $\pmb A$ in $X = \pmb A S$. Without loss of generality, we can assume that $X$ has already been whitened to have $\text{Cov}(X) = \mathbf I$, which implies that $\pmb A$ is orthogonal. Solving the ICA problem amounts to findg and orthogonal $\pmb A$ such that the components of the vector random variables $S = \pmb A^T X$ are independent and non-Gaussian.

Many of the popular approaches to ICA are based on entropy. The differential entropy $H$ of a random variable $Y$ with density $g(y)$ is given by

\begin{equation}
	H(Y) = E[-\log g(Y)] = -\int g(y)\log g(y) dy
\end{equation}

The quantity $I(Y)$ is called the \textit{Kullback-Leibler} distance or \textit{mutual information}

\begin{equation}
\begin{aligned}
		I(Y) = \sum_{j=1}^p H(Y_j) - H(Y)
\end{aligned}
\end{equation}


This is the measurement of \textit{Kullback-Leibler} distance betweenthe density $g(y)$ of $Y$ and its independence version $\prod_{j=1}^p g_j(y_j)$, where $g_j(y_j)$ is the marginal density of $Y_j$. Now, if $X$ has covariance $\mathbf I$, and $Y = \pmb A^T X$ with $\pmb A$ orthogonal, then we will have

\begin{align}
	I(Y) & = \sum_{j=1}^p H(Y_j) - H(Y) \\
	& = \sum_{j=1}^p H(Y_j) - H(X) - \log |\det \pmb A|\\
	& = \sum_{j=1}^p H(Y_j) - H(X)
\end{align}

since we have $Y \sim f_Y(y)$, and $Y = \pmb A^T X$, where we can get $X = \pmb A Y$, and the $PDF$ of $X$ is $f_X(x) = f_Y(x)||\pmb A||$

\begin{align}
	H(X) & = E[-\log f_X(x)] = -E\bigg[ \log \big(||\pmb A|| \cdot f_Y(x)\big) \bigg]\\
	& = - E\bigg[ \log ||A|| + \log f_Y(x) \bigg] \\
	& = -\log||A|| - E[-\log f_Y(x)]\\
	& = -\log||A|| + H[Y]\\
	& = H[Y]
\end{align}



























\end{document}
