\documentclass{article}
\setlength\parindent{0pt}
\setlength{\parskip}{0pt}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage[linesnumbered,boxed]{algorithm2e}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\SetKwInput{KwInt}{Initialize}
\usepackage{mathrsfs}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{geometry}
\geometry{a4paper,scale=0.8}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{ulem}
\linespread{1.5}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}


\begin{document}

\title{Review of Independent Component Analysis\\ \small{Element of Statistical Learning Book, Chapter 14}}

\author{Hanchao Zhang}

\maketitle

\section{Latent Variables an Factor Analysis}

The singular decomposition 

\begin{equation}
	\pmb X = \pmb U \pmb D \pmb V^T
\end{equation}

We can write $\pmb S = \sqrt N \pmb U$ and $\pmb A^T = \frac{\pmb D\pmb V^T}{\sqrt{N}}$

and we have 

\begin{equation}
	\pmb X = \pmb S \pmb A^T = \sqrt N \pmb U \frac{\pmb D\pmb V^T}{\sqrt{N}} = \pmb U \pmb D \pmb V^T
\end{equation}

Where $\pmb S$ and $\pmb X$ have mean $0$, and $\pmb U$ is an orthogonal matrix. We can interpret the SVD or the corresponding principal component analysis as an estimate of a latent variable model.
\begin{equation}
\begin{aligned}
	X_1 &= a_{11}S_1 + a_{12}S_2 + \cdots + a_{1p} S_p\\
	X_2 &= a_{21}S_1 + a_{22}S_2 + \cdots + a_{2p} S_p\\
	\vdots & \hspace{30mm} \vdots \\
	X_p &= a_{p1}S_1 + a_{p2}S_2 + \cdots + a_{pp} S_p
\end{aligned}
\end{equation}
However, for any orthogonal matrix $\pmb R$, we can write

\begin{equation}
\begin{aligned}
	X & = \pmb A S \\
	& = \pmb A \pmb R^T \pmb R S \\
	& = \pmb A^* S^*
\end{aligned}
\end{equation}

Hence there are many such decompositions and it is therefore impossible to identify any particular latent variable as unique underlying sources. The classical factor analysis model has the form ($q<p$)

\begin{equation}
\begin{aligned}
	X_1 &= a_{11}S_1 + a_{12}S_2 + \cdots + a_{1q} S_q + \varepsilon_1\\
	X_2 &= a_{21}S_1 + a_{22}S_2 + \cdots + a_{2q} S_q + \varepsilon_2\\
	\vdots & \hspace{30mm} \vdots \\
	X_p &= a_{p1}S_1 + a_{p2}S_2 + \cdots + a_{pq} S_q + \varepsilon_p
\end{aligned}
\end{equation}

or 

\begin{equation}
	X = \pmb A S + \varepsilon
\end{equation}


Typically the $S_j$ and $\varepsilon_j$ are modeled as Gaussian random variables and the model is fit by maximum likelihood.

\section{Independent Component Analysis}

The ICA model has the form:

\begin{equation}
\begin{aligned}
	X_1 &= a_{11}S_1 + a_{12}S_2 + \cdots + a_{1p} S_p\\
	X_2 &= a_{21}S_1 + a_{22}S_2 + \cdots + a_{2p} S_p\\
	\vdots & \hspace{30mm} \vdots \\
	X_p &= a_{p1}S_1 + a_{p2}S_2 + \cdots + a_{pp} S_p
\end{aligned}
\end{equation}

or

\begin{equation}
	X = \pmb A S
\end{equation}

where the $S_i$ are assumed to be statistically independent rather than uncorrelated. Intuitively, uncorrelated determines the second cross moment $\text{Cov(X)}$, and statistically independent determines all cross moments.

We wish to recover the matrix $\pmb A$ in $X = \pmb A S$. Without loss of generality, we can assume that $X$ has already been whitened to have $\text{Cov}(X) = \mathbf I$, which implies that $\pmb A$ is orthogonal. Solving the ICA problem amounts to findg and orthogonal $\pmb A$ such that the components of the vector random variables $S = \pmb A^T X$ are independent and non-Gaussian.

Many of the popular approaches to ICA are based on entropy. The differential entropy $H$ of a random variable $Y$ with density $g(y)$ is given by

\begin{equation}
	H(Y) = E[-\log g(Y)] = -\int g(y)\log g(y) dy
\end{equation}

The quantity $I(Y)$ is called the \textit{Kullback-Leibler} distance or \textit{mutual information}

\begin{equation}
\begin{aligned}
		I(Y) = \sum_{j=1}^p H(Y_j) - H(Y)
\end{aligned}
\end{equation}


This is the measurement of \textit{Kullback-Leibler} distance betweenthe density $g(y)$ of $Y$ and its independence version $\prod_{j=1}^p g_j(y_j)$, where $g_j(y_j)$ is the marginal density of $Y_j$. Now, if $X$ has covariance $\mathbf I$, and $Y = \pmb A^T X$ with $\pmb A$ orthogonal, then we will have

\begin{align}
	I(Y) & = \sum_{j=1}^p H(Y_j) - H(Y) \\
	& = \sum_{j=1}^p H(Y_j) - H(X) - \log |\det \pmb A|\\
	& = \sum_{j=1}^p H(Y_j) - H(X)
\end{align}


\begin{tcolorbox}[title=\textit{Kullback  Leibler divergence}, coltitle = blue!50!black, colframe = blue!25]
	$I(Y)$ is a measurement of how one probability distribution is different from another probability distribution. In our case, the two distribution is $P = g_Y(y)$ and $Q = \prod_{j=1}^p g_j(y_j)$
	
	The definition of \textit{Kullback  Leibler divergence} is

	\begin{equation}
		D_{KL}(P||Q) = \int_{\mathcal X} p(x) \frac{p(x)}{q(x)} dx
	\end{equation}
	
	which in our case becomes
	\begin{align}
		D_{KL}(P||Q) &= \int_{\mathcal X} p(x) \log \frac{p(x)}{q(x)} dx
		 = \int_{\mathcal X} \log \frac{p(x)}{q(x)} dP
		  = \int_{\mathcal Y} \log \frac{g(y)}{\prod_{j=1}^p g_j(y_j)} dG\\
		& = \int_{\mathcal Y} \bigg[ \log g(y) - \sum_{j=1}^p \log g_j(y_j) \bigg] dG\\
		 &= -E[-\log g(y)] + \sum_{j=1}^p E[-\log g_j(y_j)]\\
		 & = \sum_{j=1}^p H(Y_j) - H(Y)
	\end{align}
	
\end{tcolorbox}


since we have $Y \sim f_Y(y)$, and $Y = \pmb A^T X$, where we can get $X = \pmb A Y$, and the $PDF$ of $X$ is $f_X(x) = f_Y(x)||\pmb A||$

\begin{align}
	H(X) & = E[-\log g_X(x)] = -E\bigg[ \log \big(||\pmb A|| \cdot g_Y(x)\big) \bigg]\\
	& = - E\bigg[ \log ||A|| + \log g_Y(x) \bigg] \\
	& = -\log||A|| - E[-\log g_Y(x)]\\
	& = -\log||A|| + H[Y]\\
	& = H[Y]
\end{align}



For convenience, rather than using the entropy $H(Y_j)$, Hyvarinen and Oja (2000) ues the negentropy measure $J(Y_j)$ defined by
\vspace{1.8mm}

\begin{equation}
	J(Y_j) = H(Z_j) - H(Y_j)
\end{equation}

where $Z_j$ is a Gaussian random variable with the same variance as $Y_j$. They proposed simple approximations to negentropy which can be computed and optimized on the data.

\begin{equation}
	J(Y_j) \approx \bigg(E\big[G(Y_j)\big] - E\big[ G(Z_j) \big] \bigg)^2
\end{equation}

where $G(u) = \frac{1}{a}\log\cosh (au)$ for $1 < a < 2$.

With pre-whitened data, this amounts to looking for components that are as independent as possible.

\section{Direct Approach of Independent Component Analysis by a Joint Product Density}

Independent component have by definition a joint product density

\begin{equation}
	f_S(s) = \prod_{j = 1}^p f_j(s_j)
\end{equation}

And in the spirit of representing departures from Gaussianity, we represent each $f_j$ as

\begin{equation}
	f_j(s_j) = \phi(s_j)\exp\{g_j(s_j) \} = \frac{1}{\sqrt{2\pi}}\exp\{\frac{1}{2}s_j^2 \}\cdot \exp\{g_j(s_j) \}
\end{equation}

the log-likelihood for the observed data $X = \pmb A S$ is

\begin{equation}
	\ell(\pmb A\{g_j\}_{j=1}^p; \pmb X) = \sum_{i=1}^N\sum_{j=1}^p[\log \phi_j(a_j^Tx_i) + g_j(a_j^Tx_i) ]
\end{equation}

which we want to maximize subjected to $\pmb A$ orthogonal and $g_j$ result in density function. So, we instead maximize a regularized version 

\begin{equation}
	\sum_{j=1}^p\Big[ \sum_{i=1}^N [\log \phi_j(a_j^Tx_i) + g_j(a_j^Tx_i) ] - \underbrace{\int \phi(t)e^{g_j(t)}dt}_{\text{density}} - \underbrace{\lambda_j\int\{g_j'''(t) \}^2(t)dt}_{\text{splines penalty}}  \Big]
\end{equation}

The first integral that controls for density is problematic and requires an approximation. We construct a fine grid of $L$ values $s_\ell^*$ in increments $\Delta$ covering the observed value $s_i$ and count the number of $s_i$ in the resulting bins:

\begin{equation}
	y_\ell^* = \frac{\# s_i\in(s_i^* - \frac{\Delta}{2}, s_i^* + \frac{\Delta}{2} )}{N}
\end{equation}

Step 2 (a), we can then approximate the penalized likelihood by

\begin{equation}
	\sum_{\ell = 1}^L \Big\{ y_i^*[\log(\phi(s_\ell^*)) + g(s_\ell^*) ] - \Delta \phi(s_\ell^*)e^{g(s_\ell^*)} \Big\} - \lambda \int g'''^2(s)ds
\end{equation}

and in practice, we set all $\lambda_j$ to the same.

Step 2(b), we optimize the $\pmb A$ with respect to the penalized likelihood function. Only the first terms in the sum involve $\pmb A$, and since $\pmb A$ is orthogonal, $\phi$ do not depend on $\pmb A$. Hence, we need to maximize

\begin{equation}
	C(\pmb A) = \frac{1}{N}\sum_{j=1}^p \sum_{i=1}^N \hat g_j(a_j^Tx_i) = \sum_{j=1}^p C_j(a_j)
\end{equation}

1. for each $j$ update

\begin{equation}
	a_j \longleftarrow E\bigg\{ X\hat g_j'(a_j^TX) - E[g_j''(a_j^TX)]a_j \bigg\}
\end{equation}

2. Orthogonalize $\pmb A$ using the symmetric square-root transformation $(\pmb A\pmb A^T)^{\frac{1}{2}}\pmb A$. Let $UDV^T$ be the singular decomposition of $\pmb A$, we will have 
\begin{align}
	(\pmb A\pmb A^T)^{\frac{1}{2}}\pmb A &= (UDV^T VD^TU^T)^{\frac{1}{2}} UDV^T\\
	& = (UD^2U^T)^{\frac{1}{2}}UDV^T\\
	& = D^{-1}UDV^T\\
	 \pmb A &\leftarrow  UV^T
\end{align}















\end{document}
